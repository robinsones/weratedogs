---
title: "We Rate Dogs tweets"
output: html_document
---

# TODO WE RATE DOGS REUSES NAMES BOOO
# that's why space gets added to the end of the file, because it already existed
# no way to tell which is which, guess just have to remove duplicate names 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rtweet)
library(magick)
library(reticulate)
library(fs)
```

### Get and Resize WeRateDogs images

```{r}
weratedogs <- get_timeline("dog_rates", n = 3200)

cleaned_weratedogs <- weratedogs %>% 
  mutate(media_url = as.character(media_url)) %>%
  filter(!is.na(media_url), is.na(reply_to_status_id)) %>%
  select(text, media_url) %>%
  mutate(rating = str_extract(text, "\\d+/"), 
         name = str_extract(text, "This is [A-Za-z]+.")) %>%
  filter(!is.na(rating), 
         !is.na(name)) %>%
  mutate(name = str_remove(name, "This is ")) %>%
  mutate(name = str_remove(name, "\\."),
         rating = str_remove(rating, "/"),
         name = str_remove(name, " ")) %>%
  filter(rating <= 15) %>%
  mutate(dichotimized_rating = if_else(rating <= 13, 0, 1)) %>%
  # remove duplicate names 
  add_count(name) %>%
  filter(n == 1)

saveRDS(cleaned_weratedogs, "weratedogs_data.rds")
# cleaned_weratedogs <- readRDS("weratedogs_data.rds")
```

```{r}
walk2(cleaned_weratedogs$media_url, cleaned_weratedogs$name, 
      ~download.file(.x, paste0(.y, ".jpg")))
```

```{r}
jpg_files <- dir_ls(regexp = "\\.jpg$")
```

Make them all 750x1000

```{r}
read_scale_and_write <- function(image_name) { 
  image_name %>%
    image_read() %>%
    image_scale("750x1000!") %>%
    image_write(path = paste0("resized_images/", image_name), format = "jpg")
}
```

```{r}
dir_create("resized_images")
walk(jpg_files, read_scale_and_write)
#delete original files
walk(jpg_files, file_delete)
```

### Upload to AWS

```{r}
# You'll need set your environment with AWS variables
usethis::edit_r_environ()
#   AWS_ACCESS_KEY_ID = "abc",
#  AWS_SECRET_ACCESS_KEY = "dfg",
#  AWS_REGION = "us-east-1"

```

```{r}
# Do once 
# py_install("sagemaker-python-sdk")
# py_install("pandas")
boto3 <- import('boto3')
```

```{r}
s3 <- boto3$client('s3', 
                   aws_secret_access_key = Sys.getenv('AWS_SECRET_ACCESS_KEY'),
                   aws_access_key_id = Sys.getenv('AWS_ACCESS_KEY_ID'))
```

### Set up training and test set

```{r}
set.seed(42)
setwd("resized_images")
resized_images_files <- fs::dir_ls(regexp = "\\.jpg$")
holdout_set <- sample(resized_images_files, length(resized_images_files)/10)
fs::dir_create("holdout/image_directory")
fs::dir_create("train/image_directory")
fs::dir_create("validation/image_directory")
walk(holdout_set, ~ fs::file_move(.x, paste0( "holdout/image_directory/", .x)))
remaining_images <- setdiff(resized_images_files, holdout_set)
train_set <- sample(remaining_images, length(remaining_images)*.70)
walk(train_set, ~ fs::file_move(.x, paste0("train/image_directory/", .x)))
validation_set <- fs::dir_ls(regexp = "\\.jpg$")
walk(validation_set, ~ fs::file_move(.x, paste0("validation/image_directory/", .x)))
```

# create files with .lst

Temporary

```{r}
holdout_set <- fs::dir_ls("resized_images/holdout/image_directory")
train_set <- fs::dir_ls("resized_images/train/image_directory")
validation_set <- fs::dir_ls("resized_images/validation/image_directory")
```

```{r}
pictures_split <- tibble("file_name" = holdout_set, "location" = "holdout") %>%
  bind_rows(tibble("file_name" = validation_set, "location" = "validation")) %>%
  bind_rows(tibble("file_name" = train_set, "location" = "train")) %>%
  mutate(file_name = as.character(file_name)) %>%
  mutate(file_name = str_remove(file_name, "resized_images/validation/image_directory/"),
         file_name = str_remove(file_name, "resized_images/train/image_directory/"),
         file_name = str_remove(file_name, "resized_images/holdout/image_directory/"))

lst_info <- cleaned_weratedogs %>%
  mutate(file_name = paste0(name, ".jpg")) %>%
  inner_join(pictures_split, by = "file_name") %>%
  mutate(file_location = paste0("image_directory/", file_name),
         index = row_number()) 

train_lst <- lst_info %>%
  filter(location == "train") %>%
  select(index, dichotimized_rating, file_location) %>%
  mutate(file_location = str_remove(file_location, "image_directory/"))

validation_lst <- lst_info %>%
  filter(location == "validation") %>%
  select(index, dichotimized_rating, file_location) %>%
  mutate(file_location = str_remove(file_location, "image_directory/"))

write.table(validation_lst, file = "resized_images/validation/validation_lst.lst",
            sep = "\t", col.names = FALSE, row.names = FALSE, quote = FALSE)

write.table(train_lst, file = "resized_images/train/train_lst.lst",
            sep = "\t", col.names = FALSE, row.names = FALSE, quote = FALSE)
```


```{r}
s3$create_bucket(Bucket = "dogrates")
walk(.x = c(fs::dir_ls("holdout/image_directory"), fs::dir_ls("train/image_directory"), fs::dir_ls("validation/image_directory")),
     .f = ~ s3$put_object(Bucket = "dogrates",
                                   Body = eval(as.expression(.x)),
                                   Key = str_remove(.x, " ")))
s3$upload_file(Bucket = "dogrates",
              Filename = "resized_images/validation/validation_lst.lst", 
              Key = "validation/validation_lst.lst")
s3$upload_file(Bucket = "dogrates",
              Filename = "resized_images/train/train_lst.lst",
              Key = "train/train_lst.lst")
```

### Get model ready 

https://github.com/aws/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/imageclassification_caltech/Image-classification-fulltraining-highlevel.ipynb


weratedogs/train/all_images/happy.jpg
Log onto the console -> IAM -> Roles -> Create Role
Create a service-linked role with sagemaker.amazonaws.com
Give the role AmazonSageMakerFullAccess
Give the role AmazonS3FullAccess (<-- scope down if reasonable)
Then use the name in RoleName= like above
From https://github.com/aws/sagemaker-python-sdk/issues/300

```{r}
iam <- boto3$client('iam', 
                   aws_secret_access_key = Sys.getenv('AWS_SECRET_ACCESS_KEY'),
                   aws_access_key_id = Sys.getenv('AWS_ACCESS_KEY_ID'),
                   region_name = "us-east-1")
role_arn <- iam$get_role(RoleName = "sagemaker_role")$Role$Arn
```

```{r}
# this doesn't work, so found it manually with jupyter notebook 
#training_image <- sagemaker$image_uris$retrieve(framework = "image_classification", region ="us-east-1")
training_image <- "811284229777.dkr.ecr.us-east-1.amazonaws.com/image-classification:1"
```

## Set up model training

```{r}
sagemaker <- import('sagemaker')
sess <- sagemaker$Session(boto3$session$Session(region_name = "us-east-1"))

```

```{r}
ic <- sagemaker$estimator$Estimator(
  training_image, 
  role_arn,
  instance_count = as.integer(1),
  instance_type = "ml.p2.xlarge",
  volume_size = as.integer(50),
  input_mode = "File",
  output_path = "s3://dogrates/ic-fulltraining/output",
  sagemaker_session = sess
)
```

```{r}
# train_set <- dir_ls("resized_images/train/image_directory")
ic$set_hyperparameters(
    num_layers=as.integer(18),
    image_shape="3,750,1000",
    num_classes=as.integer(2),
    num_training_samples=length(train_set),
    mini_batch_size=as.integer(128),
    epochs=as.integer(5),
    top_k=as.integer(2),
    precision_dtype="float32"
)
```

https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/imageclassification_caltech/Image-classification-lst-format-highlevel.html
```{r}
s3train <- "s3://dogrates/train/image_directory/"
s3validation <- "s3://dogrates/validation/image_directory/"
s3train_lst <- "s3://dogrates/train/"
s3validation_lst <- "s3://dogrates/validation/"

train_data <- sagemaker$inputs$TrainingInput(
    s3_data = s3train,
    distribution="FullyReplicated",
    content_type="application/x-image",
    s3_data_type="S3Prefix"
)
validation_data <- sagemaker$inputs$TrainingInput(
    s3validation,
    distribution="FullyReplicated",
    content_type="application/x-image",
    s3_data_type="S3Prefix"
)
train_data_lst <- sagemaker$inputs$TrainingInput(
    s3train_lst,
    distribution="FullyReplicated",
    content_type="application/x-image",
    s3_data_type="S3Prefix"
)

validation_data_lst <- sagemaker$inputs$TrainingInput(
    s3validation_lst,
    distribution="FullyReplicated",
    content_type="application/x-image",
    s3_data_type="S3Prefix"
)

data_channels <- py_dict(keys = c("train", "validation", "train_lst", "validation_lst"), 
                         values = c(train_data, validation_data, train_data_lst, validation_data_lst))
```

```{r}
ic$fit(inputs = data_channels, logs = TRUE)
```

https://medium.com/data-science-bootcamp/amazon-sagemaker-ml-p2-xlarge-8b9cbc0dd7d