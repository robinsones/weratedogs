---
title: "We Rate Dogs tweets"
output: html_document
---

# TODO WE RATE DOGS REUSES NAMES BOOO
# that's why space gets added to the end of the file, because it already existed
# no way to tell which is which, guess just have to remove duplicate names 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rtweet)
library(magick)
library(reticulate)
library(fs)
```

### Get and Resize WeRateDogs images

```{r}
weratedogs <- get_timeline("dog_rates", n = 3200)

cleaned_weratedogs <- weratedogs %>% 
  mutate(media_url = as.character(media_url)) %>%
  filter(!is.na(media_url), is.na(reply_to_status_id)) %>%
  select(text, media_url) %>%
  mutate(rating = str_extract(text, "\\d+/"), 
         name = str_extract(text, "This is [A-Za-z]+.")) %>%
  filter(!is.na(rating), 
         !is.na(name)) %>%
  mutate(name = str_remove(name, "This is ")) %>%
  mutate(name = str_remove(name, "\\."),
         rating = str_remove(rating, "/"),
         name = str_remove(name, " ")) %>%
  filter(rating <= 15) %>%
  mutate(dichotimized_rating = if_else(rating <= 13, 0, 1)) %>%
  # remove duplicate names 
  add_count(name) %>%
  filter(n == 1)

saveRDS(cleaned_weratedogs, "weratedogs_data.rds")
# cleaned_weratedogs <- readRDS("weratedogs_data.rds")
```

```{r}
walk2(cleaned_weratedogs$media_url, cleaned_weratedogs$name, 
      ~download.file(.x, paste0(.y, ".jpg")))
```

```{r}
jpg_files <- dir_ls(regexp = "\\.jpg$")
```

Make them all 750x1000

```{r}
read_scale_and_write <- function(image_name) { 
  image_name %>%
    image_read() %>%
    image_scale("750x1000!") %>%
    image_write(path = paste0("resized_images/", image_name), format = "jpg")
}
```

```{r}
dir_create("resized_images")
walk(jpg_files, read_scale_and_write)
#delete original files
walk(jpg_files, file_delete)
```

### Upload to AWS

```{r}
# You'll need set your environment with AWS variables
usethis::edit_r_environ()
#   AWS_ACCESS_KEY_ID = "abc",
#  AWS_SECRET_ACCESS_KEY = "dfg",
#  AWS_REGION = "us-east-1"

```

```{r}
# Do once 
# py_install("sagemaker-python-sdk")
# py_install("pandas")
boto3 <- import('boto3')
```

```{r}
s3 <- boto3$client('s3', 
                   aws_secret_access_key = Sys.getenv('AWS_SECRET_ACCESS_KEY'),
                   aws_access_key_id = Sys.getenv('AWS_ACCESS_KEY_ID'))
```

### Set up training and test set

```{r}
set.seed(42)
setwd("resized_images")
resized_images_files <- fs::dir_ls(regexp = "\\.jpg$")
holdout_set <- sample(resized_images_files, length(resized_images_files)/10)
fs::dir_create("holdout/image_directory")
fs::dir_create("train/image_directory")
fs::dir_create("validation/image_directory")
walk(holdout_set, ~ fs::file_move(.x, paste0( "holdout/image_directory/", .x)))
remaining_images <- setdiff(resized_images_files, holdout_set)
train_set <- sample(remaining_images, length(remaining_images)*.70)
walk(train_set, ~ fs::file_move(.x, paste0("train/image_directory/", .x)))
validation_set <- fs::dir_ls(regexp = "\\.jpg$")
walk(validation_set, ~ fs::file_move(.x, paste0("validation/image_directory/", .x)))
```

# create files with .lst

Temporary

```{r}
holdout_set <- fs::dir_ls("resized_images/holdout/image_directory")
train_set <- fs::dir_ls("resized_images/train/image_directory")
validation_set <- fs::dir_ls("resized_images/validation/image_directory")
```

```{r}
pictures_split <- tibble("file_name" = holdout_set, "location" = "holdout") %>%
  bind_rows(tibble("file_name" = validation_set, "location" = "validation")) %>%
  bind_rows(tibble("file_name" = train_set, "location" = "train")) %>%
  mutate(file_name = as.character(file_name)) %>%
  mutate(file_name = str_remove(file_name, "resized_images/validation/image_directory/"),
         file_name = str_remove(file_name, "resized_images/train/image_directory/"),
         file_name = str_remove(file_name, "resized_images/holdout/image_directory/"))

lst_info <- cleaned_weratedogs %>%
  mutate(file_name = paste0(name, ".jpg")) %>%
  inner_join(pictures_split, by = "file_name") %>%
  mutate(file_location = paste0("image_directory/", file_name),
         index = row_number()) 

train_lst <- lst_info %>%
  filter(location == "train") %>%
  select(index, dichotimized_rating, file_location) 

validation_lst <- lst_info %>%
  filter(location == "validation") %>%
  select(index, dichotimized_rating, file_location)

write.table(validation_lst, file = "resized_images/validation/validation_lst.lst",
            sep = "\t", col.names = FALSE, row.names = FALSE, quote = FALSE)

write.table(train_lst, file = "resized_images/train/train_lst.lst",
            sep = "\t", col.names = FALSE, row.names = FALSE, quote = FALSE)
```


```{r}
s3$create_bucket(Bucket = "weratedogs")
walk(.x = c(fs::dir_ls("resized_images/holdout/image_directory"), fs::dir_ls("resized_images/train/image_directory"), fs::dir_ls("resized_images/validation/image_directory")),
     .f = ~ s3$upload_file(Bucket = "weratedogs",
                                   Filename = .x,
                                   Key = str_remove(.x, "resized_images/")))
s3$upload_file(Bucket = "weratedogs",
              Filename = "resized_images/validation/validation_lst.lst", 
              Key = "validation_lst/validation_lst.lst")
s3$upload_file(Bucket = "weratedogs",
              Filename = "resized_images/train/train_lst.lst",
              Key = "train_lst/train_lst.lst")
```

### Get model ready 

https://github.com/aws/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/imageclassification_caltech/Image-classification-fulltraining-highlevel.ipynb


weratedogs/train/all_images/happy.jpg
Log onto the console -> IAM -> Roles -> Create Role
Create a service-linked role with sagemaker.amazonaws.com
Give the role AmazonSageMakerFullAccess
Give the role AmazonS3FullAccess (<-- scope down if reasonable)
Then use the name in RoleName= like above
From https://github.com/aws/sagemaker-python-sdk/issues/300

```{r}
iam <- boto3$client('iam', 
                   aws_secret_access_key = Sys.getenv('AWS_SECRET_ACCESS_KEY'),
                   aws_access_key_id = Sys.getenv('AWS_ACCESS_KEY_ID'),
                   region_name = "us-east-1")
role_arn <- iam$get_role(RoleName = "sagemaker_role")$Role$Arn
```

```{r}
# this doesn't work, so found it manually with jupyter notebook 
#training_image <- sagemaker$image_uris$retrieve(framework = "image_classification", region ="us-east-1")
training_image <- "811284229777.dkr.ecr.us-east-1.amazonaws.com/image-classification:1"
```

## Set up model training

```{r}
sagemaker <- import('sagemaker')
sess <- sagemaker$Session(boto3$session$Session(region_name = "us-east-1"))

```

```{r}
ic <- sagemaker$estimator$Estimator(
  training_image, 
  role_arn,
  instance_count = as.integer(1),
  instance_type = "ml.p2.xlarge",
  volume_size = as.integer(50),
  input_mode = "File",
  output_path = "s3://dogrates/ic-fulltraining/output",
  sagemaker_session = sess
)
```

```{r}
# train_set <- dir_ls("resized_images/train/image_directory")
ic$set_hyperparameters(
    num_layers=as.integer(18),
    image_shape="3,1000,750",
    num_classes=as.integer(2),
    num_training_samples=length(train_set),
    mini_batch_size=as.integer(16),
    epochs=as.integer(5),
    top_k=as.integer(2),
    precision_dtype="float32"
)
```

https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/imageclassification_caltech/Image-classification-lst-format-highlevel.html
```{r}
s3train <- "s3://weratedogs/train/"
s3validation <- "s3://weratedogs/validation/"
s3train_lst <- "s3://weratedogs/train_lst/"
s3validation_lst <- "s3://weratedogs/validation_lst/"

train_data <- sagemaker$inputs$TrainingInput(
    s3_data = s3train,
    distribution="FullyReplicated",
    content_type="application/x-image",
    s3_data_type="S3Prefix"
)
validation_data <- sagemaker$inputs$TrainingInput(
    s3validation,
    distribution="FullyReplicated",
    content_type="application/x-image",
    s3_data_type="S3Prefix"
)
train_data_lst <- sagemaker$inputs$TrainingInput(
    s3train_lst,
    distribution="FullyReplicated",
    content_type="application/x-image",
    s3_data_type="S3Prefix"
)

validation_data_lst <- sagemaker$inputs$TrainingInput(
    s3validation_lst,
    distribution="FullyReplicated",
    content_type="application/x-image",
    s3_data_type="S3Prefix"
)

data_channels <- py_dict(keys = c("train", "validation", "train_lst", "validation_lst"), 
                         values = c(train_data, validation_data, train_data_lst, validation_data_lst))
```

Then try moving lst files into validation_lst and train_list folders instead
```{r}
ic$fit(inputs = data_channels, logs = TRUE)
```

https://medium.com/data-science-bootcamp/amazon-sagemaker-ml-p2-xlarge-8b9cbc0dd7d

have to do as.integer() otherwise get error about class float instead of class int 
```{r}
transformer <- ic$transformer(instance_count=as.integer(1), instance_type='ml.m4.xlarge')
```

```{r}
transformer$transform(s3validation)
```

You have to do a batch transform job to figure out how your model performed in terms of actual predictions - can get accuracy from the logs but maybe just predicted everything is one class. 

# look at 

```{python}
import boto3
s3_client = boto3.client("s3")
import numpy as np
import json
s3validation = "s3://weratedogs/validation/"
bucket = "weratedogs"
object_categories = ["low", "high"]
from urllib.parse import urlparse
batch_job_name = "image-classification-2021-10-09-00-48-44-562"
def list_objects(s3_client, bucket, prefix):
    response = s3_client.list_objects(Bucket=bucket, Prefix=prefix)
    objects = [content["Key"] for content in response["Contents"]]
    return objects
  
def get_label(s3_client, bucket, prefix):
    filename = prefix.split("/")[-1]
    s3_client.download_file(bucket, prefix, filename)
    with open(filename) as f:
        data = json.load(f)
        index = np.argmax(data["prediction"])
        probability = data["prediction"][index]
    return object_categories[index], probability

inputs = list_objects(s3_client, bucket, urlparse(s3validation).path.lstrip("/"))
print("Sample inputs: " + str(inputs[:2]))

outputs = list_objects(s3_client, "sagemaker-us-east-1-461249631240", batch_job_name + "/image_directory")
print("Sample output: " + str(outputs[:2]))

labels = [get_label(s3_client, "sagemaker-us-east-1-461249631240", prefix) for prefix in outputs]
```


