---
title: "We Rate Dogs tweets"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rtweet)
library(magick)
library(reticulate)
library(fs)
```

### Get and Resize WeRateDogs images

```{r}
weratedogs <- get_timeline("dog_rates", n = 3200)

cleaned_weratedogs <- weratedogs %>% 
  mutate(media_url = as.character(media_url)) %>%
  filter(!is.na(media_url), is.na(reply_to_status_id)) %>%
  select(text, media_url) %>%
  mutate(rating = str_extract(text, "\\d+/"), 
         name = str_extract(text, "This is [A-Za-z]+.")) %>%
  filter(!is.na(rating), 
         !is.na(name)) %>%
  mutate(name = str_remove_all(name, "This is |\\.|/| ")) %>%
  filter(rating <= 15) %>%
  mutate(dichotimized_rating = if_else(rating <= 13, 0, 1)) %>%
  # remove duplicate names 
  add_count(name) %>%
  filter(n == 1)

saveRDS(cleaned_weratedogs, "weratedogs_data.rds")
# cleaned_weratedogs <- readRDS("weratedogs_data.rds")
```

```{r}
# Download the images of all the dogs
walk2(cleaned_weratedogs$media_url, cleaned_weratedogs$name, 
      ~download.file(.x, paste0(.y, ".jpg")))
```

```{r}
jpg_files <- dir_ls(regexp = "\\.jpg$")
```

Make them all 750x1000

```{r}
read_scale_and_write <- function(image_name) { 
  image_name %>%
    image_read() %>%
    image_scale("750x1000!") %>%
    image_write(path = paste0("resized_images/", image_name), format = "jpg")
}
```

```{r}
dir_create("resized_images")
walk(jpg_files, read_scale_and_write)
#delete original files
walk(jpg_files, file_delete)
```

### Set up training, validation, and holdout set

```{r}
set.seed(42)
# Sorry Jenny Bryan
setwd("resized_images")
dir_create("holdout/image_directory")
dir_create("train/image_directory")
dir_create("validation/image_directory")

resized_images_files <- dir_ls(regexp = "\\.jpg$")
holdout_set <- sample(resized_images_files, length(resized_images_files)/10)
walk(holdout_set, ~ file_move(.x, paste0( "holdout/image_directory/", .x)))

remaining_images <- setdiff(resized_images_files, holdout_set)
train_set <- sample(remaining_images, length(remaining_images)*.70)
walk(train_set, ~ file_move(.x, paste0("train/image_directory/", .x)))

# move remaining files to validation folder
validation_set <- dir_ls(regexp = "\\.jpg$")
walk(validation_set, ~ file_move(.x, paste0("validation/image_directory/", .x)))
```

# create files with .lst

```{r}
# run if you've restarted since creating holdout train and validation set
holdout_set <- dir_ls("resized_images/holdout/image_directory")
train_set <- dir_ls("resized_images/train/image_directory")
validation_set <- dir_ls("resized_images/validation/image_directory")
```

```{r}
pictures_split <- tibble("file_name" = holdout_set, "location" = "holdout") %>%
  bind_rows(tibble("file_name" = validation_set, "location" = "validation")) %>%
  bind_rows(tibble("file_name" = train_set, "location" = "train")) %>%
  mutate(file_name = as.character(file_name),
         file_name = str_remove_all(file_name,"resized_images/validation/image_directory/|resized_images/train/image_directory/|resized_images/holdout/image_directory/"))

lst_info <- cleaned_weratedogs %>%
  mutate(file_name = paste0(name, ".jpg")) %>%
  inner_join(pictures_split, by = "file_name") %>%
  mutate(file_location = paste0("image_directory/", file_name),
         index = row_number()) 

train_lst <- lst_info %>%
  filter(location == "train") %>%
  select(index, dichotimized_rating, file_location) 

validation_lst <- lst_info %>%
  filter(location == "validation") %>%
  select(index, dichotimized_rating, file_location)

write.table(validation_lst, file = "resized_images/validation/validation_lst.lst",
            sep = "\t", col.names = FALSE, row.names = FALSE, quote = FALSE)

write.table(train_lst, file = "resized_images/train/train_lst.lst",
            sep = "\t", col.names = FALSE, row.names = FALSE, quote = FALSE)
```

### Upload to AWS

```{r}
# You'll need to set your environment with AWS variables. Add the commented lines (without the #) into your R environment, replacing "abc" and "dfg" with your keys
usethis::edit_r_environ()
#  AWS_ACCESS_KEY_ID = "abc",
#  AWS_SECRET_ACCESS_KEY = "dfg",
#  AWS_REGION = "us-east-1"
```

```{r}
# Only need to install the packages once 
py_install("sagemaker-python-sdk")
py_install("pandas")
boto3 <- import('boto3')
```

```{r}
s3 <- boto3$client('s3', 
                   aws_secret_access_key = Sys.getenv('AWS_SECRET_ACCESS_KEY'),
                   aws_access_key_id = Sys.getenv('AWS_ACCESS_KEY_ID'))

s3$create_bucket(Bucket = "weratedogs")
```

```{r}
all_images <- c(dir_ls("resized_images/holdout/image_directory"),
                dir_ls("resized_images/train/image_directory"),
                dir_ls("resized_images/validation/image_directory"))

walk(.x = all_images,
     .f = ~ s3$upload_file(Bucket = "weratedogs",
                           Filename = .x,
                           Key = str_remove(.x, "resized_images/")))
```

```{r}
s3$upload_file(Bucket = "weratedogs",
              Filename = "resized_images/validation/validation_lst.lst", 
              Key = "validation_lst/validation_lst.lst")

s3$upload_file(Bucket = "weratedogs",
              Filename = "resized_images/train/train_lst.lst",
              Key = "train_lst/train_lst.lst")
```

### Get model ready 
To create an IAM Role with the right permissions: 
- Log onto the console -> IAM -> Roles -> Create Role
- Create a service-linked role with sagemaker.amazonaws.com
- Give the role AmazonSageMakerFullAccess
- Give the role AmazonS3FullAccess 
- Name is "sagemaker_role"
From https://github.com/aws/sagemaker-python-sdk/issues/300

```{r}
iam <- boto3$client('iam', 
                   aws_secret_access_key = Sys.getenv('AWS_SECRET_ACCESS_KEY'),
                   aws_access_key_id = Sys.getenv('AWS_ACCESS_KEY_ID'),
                   region_name = "us-east-1")

role_arn <- iam$get_role(RoleName = "sagemaker_role")$Role$Arn
```

```{r}
# commented code below doesn't work, so found it manually with a jupyter notebook 
# training_image <- sagemaker$image_uris$retrieve(framework = "image_classification", region ="us-east-1")
training_image <- "811284229777.dkr.ecr.us-east-1.amazonaws.com/image-classification:1"
```

## Set up model training

```{r}
sagemaker <- import('sagemaker')
sess <- sagemaker$Session(boto3$session$Session(region_name = "us-east-1"))
```

```{r}
ic <- sagemaker$estimator$Estimator(
  training_image, 
  role_arn,
  instance_count = as.integer(1),
  instance_type = "ml.p2.xlarge",
  volume_size = as.integer(50),
  input_mode = "File",
  output_path = "s3://dogrates/ic-fulltraining/output",
  sagemaker_session = sess
)
```

```{r}
ic$set_hyperparameters(
    num_layers=as.integer(18),
    image_shape="3,1000,750",
    num_classes=as.integer(2),
    num_training_samples=length(train_set),
    mini_batch_size=as.integer(16),
    epochs=as.integer(5),
    top_k=as.integer(2),
    precision_dtype="float32"
)
```

```{r}
training_input <- function(s3_data) {
  sagemaker$inputs$TrainingInput(
    s3_data,
    distribution="FullyReplicated",
    content_type="application/x-image",
    s3_data_type="S3Prefix"
  )
}

train_data <- training_input("s3://weratedogs/train/")
validation_data <- training_input("s3://weratedogs/validation/")
train_data_lst <- training_input("s3://weratedogs/train_lst/")
validation_data_lst <- training_input("s3://weratedogs/validation_lst/")

data_channels <- py_dict(keys = c("train", "validation", "train_lst", "validation_lst"), 
                         values = c(train_data, validation_data, train_data_lst, validation_data_lst))
```

Need to follow instructions on https://medium.com/data-science-bootcamp/amazon-sagemaker-ml-p2-xlarge-8b9cbc0dd7d to get access to machine that can be used with the image classification model. 

```{r}
ic$fit(inputs = data_channels, logs = TRUE)
```

### Get Predictions

You have to do a batch transform job to figure out how your model performed in terms of actual predictions - can get accuracy from the logs but maybe just predicted everything is one class. 

```{r}
transformer <- ic$transformer(instance_count=as.integer(1), instance_type='ml.m4.xlarge')
transformer$transform("s3://weratedogs/validation/")
```

```{python}
import boto3
import numpy as np
import json
from urllib.parse import urlparse

s3_client = boto3.client("s3")
s3validation = "s3://weratedogs/validation/"
bucket = "weratedogs"
object_categories = ["low", "high"]
batch_job_name = "image-classification-2021-10-09-00-48-44-562"

def list_objects(s3_client, bucket, prefix):
    response = s3_client.list_objects(Bucket=bucket, Prefix=prefix)
    objects = [content["Key"] for content in response["Contents"]]
    return objects
  
def get_label(s3_client, bucket, prefix):
    filename = prefix.split("/")[-1]
    s3_client.download_file(bucket, prefix, filename)
    with open(filename) as f:
        data = json.load(f)
        index = np.argmax(data["prediction"])
        probability = data["prediction"][index]
    return object_categories[index], probability

inputs = list_objects(s3_client, bucket, urlparse(s3validation).path.lstrip("/"))
print("Sample inputs: " + str(inputs[:2]))

outputs = list_objects(s3_client, "sagemaker-us-east-1-461249631240", batch_job_name + "/image_directory")
print("Sample output: " + str(outputs[:2]))

labels = [get_label(s3_client, "sagemaker-us-east-1-461249631240", prefix) for prefix in outputs]

[x[0] for x in labels]
```
